{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:17:04.595472Z","iopub.execute_input":"2021-11-03T07:17:04.595950Z","iopub.status.idle":"2021-11-03T07:17:05.384270Z","shell.execute_reply.started":"2021-11-03T07:17:04.595850Z","shell.execute_reply":"2021-11-03T07:17:05.383284Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Wed Nov  3 07:17:05 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# DON\"T USE WANDB, theres a bug with it handling distributed training\n#!pip uninstall -y wandb\n!pip install --upgrade wandb\n\n#INSTALLING WANDB ALTERNATIVE\n#!pip install comet_ml\n\n## INSTALLING TRANSFORMERS FROM SOURCE\n!pip install git+https://github.com/huggingface/transformers.git@master\n#!pip install --upgrade transformers\n\n!pip install --upgrade numpy tokenizers rouge_score tqdm\n!pip install datasets\n\n## INSTALLING PyTorch/XLA using pip\n#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n\n## INSTALLING PyTorch/XLA from source\n#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n#!python pytorch-xla-env-setup.py --version 1.9 --apt-packages libomp5 libopenblas-dev\n\n#!pip install git+https://github.com/pytorch/pytorch.git@master\n#!pip install --upgrade torch\n\n#!pip install --upgrade deepspeed\n!pip install git+https://github.com/microsoft/DeepSpeed.git@master","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:24:02.615499Z","iopub.execute_input":"2021-11-03T07:24:02.615867Z","iopub.status.idle":"2021-11-03T07:25:33.171145Z","shell.execute_reply.started":"2021-11-03T07:24:02.615786Z","shell.execute_reply":"2021-11-03T07:25:33.170145Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.10.33)\nCollecting wandb\n  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n\u001b[K     |████████████████████████████████| 1.7 MB 896 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.1)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.4.1)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.17.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.2.0)\nCollecting yaspin>=1.0.0\n  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (7.1.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.18)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\nRequirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\nRequirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.5)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nInstalling collected packages: yaspin, wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.10.33\n    Uninstalling wandb-0.10.33:\n      Successfully uninstalled wandb-0.10.33\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.5.0 requires wandb<0.11.0,>=0.10.0, but you have wandb 0.12.6 which is incompatible.\u001b[0m\nSuccessfully installed wandb-0.12.6 yaspin-2.1.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting git+https://github.com/huggingface/transformers.git@master\n  Cloning https://github.com/huggingface/transformers.git (to revision master) to /tmp/pip-req-build-vntxnbo8\n  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-vntxnbo8\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (0.0.45)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (5.4.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (3.0.12)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (4.61.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (20.9)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (3.4.0)\nCollecting huggingface-hub<1.0,>=0.1.0\n  Downloading huggingface_hub-0.1.0-py3-none-any.whl (59 kB)\n\u001b[K     |████████████████████████████████| 59 kB 818 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (2.25.1)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (0.10.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (2021.4.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.13.0.dev0) (1.19.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.4.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.13.0.dev0) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.13.0.dev0) (1.26.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.13.0.dev0) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3077935 sha256=f06892287a7c0352a1f02be62129fb4dee5994a36048b624fad911f9385c103d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-lfxus79c/wheels/bf/7e/a0/2294517bde7567f13bb134196852402df9b60eeab329d9581e\nSuccessfully built transformers\nInstalling collected packages: huggingface-hub, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.0.8\n    Uninstalling huggingface-hub-0.0.8:\n      Successfully uninstalled huggingface-hub-0.0.8\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.6.1\n    Uninstalling transformers-4.6.1:\n      Successfully uninstalled transformers-4.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.5.0 requires transformers<4.7,>=4.1, but you have transformers 4.13.0.dev0 which is incompatible.\nallennlp 2.5.0 requires wandb<0.11.0,>=0.10.0, but you have wandb 0.12.6 which is incompatible.\u001b[0m\nSuccessfully installed huggingface-hub-0.1.0 transformers-4.13.0.dev0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.19.5)\nCollecting numpy\n  Downloading numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[K     |████████████████████████████████| 15.7 MB 904 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.10.3)\nCollecting rouge_score\n  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.61.1)\nCollecting tqdm\n  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n\u001b[K     |████████████████████████████████| 76 kB 5.2 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge_score) (0.12.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.15.0)\nInstalling collected packages: numpy, tqdm, rouge-score\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.5\n    Uninstalling numpy-1.19.5:\n      Successfully uninstalled numpy-1.19.5\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.61.1\n    Uninstalling tqdm-4.61.1:\n      Successfully uninstalled tqdm-4.61.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\nfancyimpute 0.5.5 requires tensorflow, which is not installed.\ndask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\ncudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.3 which is incompatible.\ntensorflow-gpu 2.4.1 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.2 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.17.3 which is incompatible.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.21.3 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\nallennlp 2.5.0 requires transformers<4.7,>=4.1, but you have transformers 4.13.0.dev0 which is incompatible.\nallennlp 2.5.0 requires wandb<0.11.0,>=0.10.0, but you have wandb 0.12.6 which is incompatible.\u001b[0m\nSuccessfully installed numpy-1.21.3 rouge-score-0.0.4 tqdm-4.62.3\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting datasets\n  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n\u001b[K     |████████████████████████████████| 290 kB 893 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.9)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.1.0)\nRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\nCollecting xxhash\n  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 13.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.6.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.3)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.5)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.1.0)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.6.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: xxhash, datasets\nSuccessfully installed datasets-1.15.1 xxhash-2.0.2\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting git+https://github.com/microsoft/DeepSpeed.git@master\n  Cloning https://github.com/microsoft/DeepSpeed.git (to revision master) to /tmp/pip-req-build-a9gk0q6_\n  Running command git clone -q https://github.com/microsoft/DeepSpeed.git /tmp/pip-req-build-a9gk0q6_\n  Running command git submodule update --init --recursive -q\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.5.6+426dd2b) (1.7.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.5.6+426dd2b) (4.62.3)\nCollecting tensorboardX==1.8\n  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n\u001b[K     |████████████████████████████████| 216 kB 889 kB/s eta 0:00:01\n\u001b[?25hCollecting ninja\n  Downloading ninja-1.10.2.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n\u001b[K     |████████████████████████████████| 108 kB 8.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.5.6+426dd2b) (1.21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.5.6+426dd2b) (5.8.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.5.6+426dd2b) (20.9)\nCollecting triton\n  Downloading triton-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[K     |████████████████████████████████| 18.2 MB 8.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorboardX==1.8->deepspeed==0.5.6+426dd2b) (1.15.0)\nRequirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX==1.8->deepspeed==0.5.6+426dd2b) (3.17.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->deepspeed==0.5.6+426dd2b) (2.4.7)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->deepspeed==0.5.6+426dd2b) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->deepspeed==0.5.6+426dd2b) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->deepspeed==0.5.6+426dd2b) (0.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from triton->deepspeed==0.5.6+426dd2b) (3.0.12)\nBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.5.6+426dd2b-py3-none-any.whl size=518257 sha256=e63fd819364b8ad104ced78d87744fd658775a32c6ebc1a38f0160cda70b490c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mw7fb0gh/wheels/b0/3c/cb/b08cb924ca53ee7a927fb5d2ba0269afa1c650ff1ff648be7a\nSuccessfully built deepspeed\nInstalling collected packages: triton, tensorboardX, ninja, deepspeed\n  Attempting uninstall: tensorboardX\n    Found existing installation: tensorboardX 2.2\n    Uninstalling tensorboardX-2.2:\n      Successfully uninstalled tensorboardX-2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.5.0 requires transformers<4.7,>=4.1, but you have transformers 4.13.0.dev0 which is incompatible.\nallennlp 2.5.0 requires wandb<0.11.0,>=0.10.0, but you have wandb 0.12.6 which is incompatible.\u001b[0m\nSuccessfully installed deepspeed-0.5.6+426dd2b ninja-1.10.2.2 tensorboardX-1.8 triton-1.1.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from subprocess import run\nUSE_DEEPSPEED_OR_TPU = \"DEEPSPEED\" ## OPTIONS: \"DEEPSPEED\", \"TPU\", \"NONE\"\nPY_TPU_VERSION = \"1.10\"\nMONITORING_SYSTEM = \"Wandb\" ## OPTIONS: \"Wandb\", \"Cometml\"\n\n### INITIALIZING ENVIRONMENT VARIABLES FOR TPU OR DEEPSPEED:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nif USE_DEEPSPEED_OR_TPU==\"TPU\":\n    os.environ[\"XLA_USE_BF16\"] = \"1\"\n    \nelif USE_DEEPSPEED_OR_TPU==\"DEEPSPEED\":\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n    os.environ['RANK'] = \"0\"\n    os.environ['LOCAL_RANK'] = \"0\"\n    os.environ['WORLD_SIZE'] = \"1\"\n\nimport gc\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhuggingface_hub_token = user_secrets.get_secret(\"huggingface-hub-token\")\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"wandb-key\")\nos.environ[\"COMET_API_KEY\"] = user_secrets.get_secret(\"comet-key\")\n\nif MONITORING_SYSTEM == \"Cometml\":\n    import comet_ml\nelse:\n    import wandb\nfrom tqdm.notebook import tqdm\nimport transformers\nimport tokenizers\n#import datasets\nimport tensorflow as tf\nimport glob\nimport numpy as np\nimport json\nimport pandas as pd\nfrom functools import partial\n\nfrom datasets import (\n    load_dataset,\n    load_metric,\n    load_from_disk,\n)\n\nfrom transformers import(\n    Trainer,\n    TrainingArguments,\n    is_torch_tpu_available,\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoModel,\n    DataCollatorWithPadding,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\nfrom transformers.modeling_outputs import Seq2SeqLMOutput\nfrom transformers.optimization import (\n    Adafactor,\n    AdafactorSchedule,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\n\nfrom dataclasses import dataclass, field\nimport nltk\nnltk.download(\"punkt\")\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\n\nfrom kaggle_datasets import KaggleDatasets\n\nif is_torch_tpu_available():\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    import torch_xla.utils.serialization as xser\nelse:\n    import deepspeed\n\nfrom typing import Dict, List, Optional\n\nfrom subprocess import run\n\n\nrouge_metric = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:27.099610Z","iopub.execute_input":"2021-11-03T07:27:27.099974Z","iopub.status.idle":"2021-11-03T07:27:36.037036Z","shell.execute_reply.started":"2021-11-03T07:27:27.099937Z","shell.execute_reply":"2021-11-03T07:27:36.036281Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcc266bd3e5f4f148b33baeeb5fd9028"}},"metadata":{}}]},{"cell_type":"code","source":"from platform import python_version\nprint(python_version())\nprint(tf.__version__)\nprint(transformers.__version__)\nprint(torch.__version__)\n\nif not is_torch_tpu_available():\n    print(deepspeed.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:36.038668Z","iopub.execute_input":"2021-11-03T07:27:36.039025Z","iopub.status.idle":"2021-11-03T07:27:36.047927Z","shell.execute_reply.started":"2021-11-03T07:27:36.038987Z","shell.execute_reply":"2021-11-03T07:27:36.047113Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"3.7.10\n2.4.1\n4.13.0.dev0\n1.7.0\n0.5.6+426dd2b\n","output_type":"stream"}]},{"cell_type":"code","source":"TPU_CORES=0\nN_GPUS=0\n\nif is_torch_tpu_available():\n    TPU_CORES = 8\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 32\n    GRAD_ACCU_STEPS = 1\n    FP16 = False\n    DS_Config=None\nelif USE_DEEPSPEED_OR_TPU==\"DEEPSPEED\":\n    N_GPUS = 1\n    TRAIN_BATCH_SIZE = 10\n    VALID_BATCH_SIZE = 10\n    GRAD_ACCU_STEPS = 1\n    FP16=True\n    DS_Config=\"zero3\"\n    #DS_Config=\"zero3_infinity\"\n    #DS_Config=\"zero2\"\nelse:\n    N_GPUS = 1\n    TRAIN_BATCH_SIZE = 4\n    VALID_BATCH_SIZE = 4\n    GRAD_ACCU_STEPS = 1\n    FP16=True\n    DS_Config=None\n    \nclass ConfigInitializer:\n    \n    ## DATA CONFIGS\n    use_fast_tokenizer=True\n    #prefix=\"summarize: \"\n    prefix=\"\"\n    \n    valid_test_tpu_support=True\n    train_required_columns=[\"input_ids\", \"attention_mask\",\n                            \"decoder_attention_mask\",\n                            \"labels\"]\n    eval_required_columns = [\"input_ids\", \"attention_mask\", \n                             \"decoder_attention_mask\",\n                             \"labels\"]\n    max_input_len=256\n    max_output_len=64\n    truncation=True\n    padding=\"max_length\"\n    return_tensors=\"pt\"\n    input_column=\"document\"\n    output_column=\"summary\"\n\n    train_batch_size=TRAIN_BATCH_SIZE\n    valid_batch_size=VALID_BATCH_SIZE\n    test_batch_size=4\n\n    drop_last_train=True\n    drop_last_valid=False\n    drop_last_test=False\n\n    num_workers_train=2\n    num_workers_valid=2\n    num_workers_test=2\n\n    datasets_main_dir=os.path.join(os.curdir, \"my_datasets\")\n    train_ds_path=os.path.join(datasets_main_dir, \"train_ds.pt\")\n    valid_ds_path=os.path.join(datasets_main_dir, \"valid_ds.pt\")\n    test_ds_path=os.path.join(datasets_main_dir, \"test_ds.pt\")\n    \n    ## TRAINING CONFIGS\n    max_steps=1000\n    optimizer_scale_parameter=False\n    optimizer_relative_step=False\n    optimizer_warmup_init=False\n    lr=1e-3\n    initial_lr=1e-3\n    epochs=1\n    logging_ratio = 0.01\n    warmup_ratio=0.1\n    saving_ratio = 0.2\n    eval_ratio = 0.1\n    total_train_steps=100\n    logging_steps=None\n    eval_steps=None\n    save_steps=None\n    save_strategy=\"no\"\n    evaluation_strategy=\"steps\"\n    output_dir=os.path.join(os.curdir, \"results\")\n    logging_dir=os.path.join(os.curdir, \"log\")\n    weight_decay=0.0\n    num_beams=4\n    skip_special_tokens_in_decode=True\n    use_stemmer_for_compute=True\n    adam_epsilon=1e-8\n    gradient_accumulation_steps=GRAD_ACCU_STEPS\n    n_gpu=N_GPUS\n    tpu_cores=TPU_CORES\n    seed=42\n    use_ada_factor=True\n    compute_metrics=False\n    fp16=FP16\n    rouge_before_training=False\n    limited_host_mem=False\n    use_ds_optimizer_scheduler=False\n    \n    def __init__(self, gcs_path, project_name, model_checkpoint,\n                 model_name,dataset_name, dataset_dir, csv_folder=None, train_len=None,\n                 valid_len=200, test_len=20, monitoring_system=\"Wandb\"):\n        self.dataset_name = dataset_name\n        self.project_name = project_name\n\n        if csv_folder:\n            self.train_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_train.csv\")\n            self.valid_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_valid.csv\")\n            self.test_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_test.csv\")\n        else:\n            self.train_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_train.csv\")\n            self.valid_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_valid.csv\")\n            self.test_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_test.csv\")\n\n        self.model_checkpoint = model_checkpoint\n        self.model_name = model_name\n\n        self.xser_save_path=f\"{model_name.replace('-', '_')}_{dataset_name}\"\n        self.config_json_path=f\"{self.xser_save_path}.json\"\n        self.tokenizer_save_path=f\"{self.xser_save_path}_tokenizer\"\n\n        train_file_path = os.path.join(os.curdir,f\"{self.dataset_name}_train.csv\")\n        valid_file_path = os.path.join(os.curdir,f\"{self.dataset_name}_valid.csv\")\n        test_file_path  = os.path.join(os.curdir,f\"{self.dataset_name}_test.csv\")\n        \n        train_file_check = os.path.exists(train_file_path)\n        valid_file_check = os.path.exists(valid_file_path)\n        test_file_check = os.path.exists(test_file_path)\n        \n        file_check_list = [(self.train_files, train_file_check),\n                           (self.valid_files, valid_file_check),\n                           (self.test_files, test_file_check)]\n\n        for data_file, file_check in file_check_list:\n            if not file_check:\n                run([\"gsutil\", \"cp\", data_file, \".\"])\n\n        self.train_files = os.path.join(os.curdir,f\"{self.dataset_name}_train.csv\")\n        self.valid_files = os.path.join(os.curdir,f\"{self.dataset_name}_valid.csv\")\n        self.test_files = os.path.join(os.curdir,f\"{self.dataset_name}_test.csv\")\n        \n\n        self.train_len=train_len\n        self.valid_len=valid_len\n        self.test_len=test_len\n        self.total_train_step=None\n        \n        if DS_Config:\n            self.deepspeed_config = os.path.join(os.curdir, f\"ds_config_{DS_Config}.json\")\n        else:\n            self.deepspeed_config = DS_Config\n\n        if monitoring_system == \"Cometml\":\n            comet_ml.init(project_name=self.project_name)\n        else:\n            pass\n            #wandb.init(project=self.project_name)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:36.049708Z","iopub.execute_input":"2021-11-03T07:27:36.050217Z","iopub.status.idle":"2021-11-03T07:27:36.082986Z","shell.execute_reply.started":"2021-11-03T07:27:36.050179Z","shell.execute_reply":"2021-11-03T07:27:36.082018Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## DeepSpeed Config files:","metadata":{}},{"cell_type":"markdown","source":"    ## If you don't want to use AdaFactor then, copy the below dict into config json\n    \n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    \n    ## And remove the below line from config json:\n    \n    \"zero_allow_untested_optimizer\": true","metadata":{}},{"cell_type":"markdown","source":"### Zero2:","metadata":{}},{"cell_type":"markdown","source":"    zero2_config_dict = {\n        \"fp16\": {\n            \"enabled\": \"auto\",\n            \"loss_scale\": 0,\n            \"loss_scale_window\": 1000,\n            \"initial_scale_power\": 16,\n            \"hysteresis\": 2,\n            \"min_loss_scale\": 1\n        },\n\n        \"zero_allow_untested_optimizer\": True,\n\n        \"\"\"\n        \"optimizer\": {\n            \"type\": \"AdamW\",\n            \"params\": {\n                \"lr\": \"auto\",\n                \"betas\": \"auto\",\n                \"eps\": \"auto\",\n                \"weight_decay\": \"auto\"\n            }\n        },\n        \"\"\"\n        \"\"\"\n        \"scheduler\": {\n            \"type\": \"WarmupLR\",\n            \"params\": {\n                \"warmup_min_lr\": \"auto\",\n                \"warmup_max_lr\": \"auto\",\n                \"warmup_num_steps\": \"auto\"\n            }\n        },\n        \"\"\"\n\n        \"zero_optimization\": {\n            \"stage\": 2,\n            \"\"\"\n            \"offload_optimizer\": {\n                \"device\": None,\n                \"pin_memory\": True\n            },\n            \"\"\"\n            \"allgather_partitions\": True,\n            \"allgather_bucket_size\": 5e8,\n            \"overlap_comm\": True,\n            \"reduce_scatter\": True,\n            \"reduce_bucket_size\": 5e8,\n            \"contiguous_gradients\": True\n        },\n\n        \"gradient_accumulation_steps\": \"auto\",\n        \"gradient_clipping\": \"auto\",\n        \"steps_per_print\": 2000,\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": TRAIN_BATCH_SIZE,\n        \"wall_clock_breakdown\": False\n    }\n\n    with open(\"ds_config_zero2.json\", \"w\") as config_file:\n        json.dump(zero2_config_dict, config_file)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:02:05.425936Z","iopub.execute_input":"2021-11-02T17:02:05.426564Z","iopub.status.idle":"2021-11-02T17:02:05.438282Z","shell.execute_reply.started":"2021-11-02T17:02:05.426516Z","shell.execute_reply":"2021-11-02T17:02:05.437232Z"}}},{"cell_type":"markdown","source":"        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },","metadata":{}},{"cell_type":"code","source":"zero3_config_dict = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \n    \"zero_allow_untested_optimizer\": True,\n\n    \"\"\"\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"\"\"\n    \"\"\"\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    \"\"\"\n    \n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": False\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"overlap_comm\": False,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e5,\n        \"allgather_bucket_size\": 1e8,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e5,\n        \"stage3_max_reuse_distance\": 1e5,\n        \"stage3_gather_fp16_weights_on_model_save\": True\n    },\n    \n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}\n\n\nwith open(\"ds_config_zero3.json\", \"w\") as config_file:\n    json.dump(zero3_config_dict, config_file)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:36.085741Z","iopub.execute_input":"2021-11-03T07:27:36.086280Z","iopub.status.idle":"2021-11-03T07:27:36.095953Z","shell.execute_reply.started":"2021-11-03T07:27:36.086229Z","shell.execute_reply":"2021-11-03T07:27:36.095088Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Zero3-infinity:","metadata":{}},{"cell_type":"code","source":"zero3_infinity_config_dict = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \n    \"zero_allow_untested_optimizer\": True,\n    \n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": True,\n            \"buffer_count\": 4,\n            \"fast_init\": False\n        },\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": True,\n            \"buffer_count\": 5,\n            \"buffer_size\": 1e8,\n            \"max_in_cpu\": 1e9\n        },\n        \"aio\": {\n            \"block_size\": 262144,\n            \"queue_depth\": 32,\n            \"thread_count\": 1,\n            \"single_submit\": False,\n            \"overlap_events\": True\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_fp16_weights_on_model_save\": True\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}\n\nwith open(\"ds_config_zero3_infinity.json\", \"w\") as config_file:\n    json.dump(zero3_infinity_config_dict, config_file)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:36.097193Z","iopub.execute_input":"2021-11-03T07:27:36.097633Z","iopub.status.idle":"2021-11-03T07:27:36.108453Z","shell.execute_reply.started":"2021-11-03T07:27:36.097595Z","shell.execute_reply":"2021-11-03T07:27:36.107107Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path(\"sci-summarizer-data\")\nprint(GCS_PATH)\n\nConfig = ConfigInitializer(project_name=\"t5_large_xsum\",\n                           monitoring_system=MONITORING_SYSTEM,\n                           #model_checkpoint=\"microsoft/prophetnet-large-uncased-cnndm\",\n                           #model_checkpoint=\"google/pegasus-cnn_dailymail\",\n                           #model_checkpoint=\"facebook/bart-large-cnn\",\n                           model_checkpoint=\"t5-large\",\n                           #model_checkpoint=\"t5-base\",\n                           #model_checkpoint=\"t5-small\",\n                           \n                           #model_name=\"prophetnet\",\n                           #model_name=\"pegasus\",\n                           #model_name=\"bart-large\",\n                           #model_name=\"bart\",\n                           model_name=\"t5-large\",\n                           #model_name=\"t5-base\",\n                           #model_name=\"t5-small\",\n                           \n                           dataset_name=\"xsum\",\n                           gcs_path=GCS_PATH,\n                           dataset_dir=\"xsum_data\",\n                           train_len=None,\n                           valid_len=200,\n                           test_len=20)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:36.110273Z","iopub.execute_input":"2021-11-03T07:27:36.111086Z","iopub.status.idle":"2021-11-03T07:27:47.835850Z","shell.execute_reply.started":"2021-11-03T07:27:36.111045Z","shell.execute_reply":"2021-11-03T07:27:47.834612Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"gs://kds-6abe672a34848b32096cc0bc1d10fef197e6f9ee6df5b1391c6a7bf9\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_rouge(model, tokenizer,\n                  tokenized_dataset,\n                  gen_kwargs=None,\n                  Config=Config):\n    \n    def postprocess_text(preds, labels):\n        preds = [pred.strip() for pred in preds]\n        labels = [label.strip() for label in labels]\n\n        # rougeLSum expects newline after each sentence\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n        return preds, labels\n    \n\n    for batch in tqdm(tokenized_dataset,\n                      total=len(tokenized_dataset),\n                      unit=\"batchs\"):\n        \n        if is_torch_tpu_available():\n            temp_batch = {\n                \"input_ids\": batch[\"input_ids\"],\n                \"attention_mask\": batch[\"attention_mask\"],\n            }\n        else:\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].to(\"cuda\"),\n                \"attention_mask\": batch[\"attention_mask\"].to(\"cuda\"),\n            }\n            \n        labels = batch[\"labels\"]\n        temp_batch.update(gen_kwargs)\n        generated_tokens = model.generate(**temp_batch)\n\n        if isinstance(generated_tokens, tuple):\n            generated_tokens = generated_tokens[0]\n\n\n        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=Config.skip_special_tokens_in_decode)\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=Config.skip_special_tokens_in_decode)\n        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n        rouge_metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n\n    result = rouge_metric.compute(use_stemmer=Config.use_stemmer_for_compute)\n\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:47.837652Z","iopub.execute_input":"2021-11-03T07:27:47.838015Z","iopub.status.idle":"2021-11-03T07:27:47.860114Z","shell.execute_reply.started":"2021-11-03T07:27:47.837973Z","shell.execute_reply":"2021-11-03T07:27:47.859319Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Initializing the tokenizer and tokenizing the data:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(Config.model_checkpoint, use_fast=Config.use_fast_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:47.862946Z","iopub.execute_input":"2021-11-03T07:27:47.863512Z","iopub.status.idle":"2021-11-03T07:27:53.641116Z","shell.execute_reply.started":"2021-11-03T07:27:47.863471Z","shell.execute_reply":"2021-11-03T07:27:53.640266Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a1975eb26147b6918a667d22712295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557d5d2463a94afa81482684c9dac079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d038cd8a99d14108afb4da0fc87b0f8f"}},"metadata":{}}]},{"cell_type":"code","source":"class MyXSum(Dataset):\n    \n    def __init__(self, Config, tokenizer, main_ds, split_type):  \n        \n        self.model_name = Config.model_checkpoint\n        self.dataset = main_ds[split_type]\n        self.tokenizer = tokenizer\n        self.Config = Config\n        \n        if split_type in set([\"validation\", \"test\"]):\n            self.required_columns = Config.eval_required_columns\n            if split_type == \"validation\":\n                num_samples = Config.valid_len\n            else:\n                num_samples = Config.test_len\n        else:\n            self.required_columns = Config.train_required_columns\n            num_samples = Config.train_len\n            \n        if num_samples:\n            self.dataset = self.dataset.select(list(range(0, num_samples)))\n  \n    def __len__(self):\n        return self.dataset.shape[0]\n    \n    def preprocess_function(self, examples):\n\n        _inputs = [self.Config.prefix + examples[self.Config.input_column]]\n        _target = [examples[self.Config.output_column]]\n        \n        model_inputs = self.tokenizer(_inputs, max_length=self.Config.max_input_len,\n                                      truncation=self.Config.truncation, padding=self.Config.padding,\n                                      return_tensors=self.Config.return_tensors)\n\n        # Setup the tokenizer for targets\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(_target, max_length=self.Config.max_output_len,\n                                    truncation=self.Config.truncation, padding=self.Config.padding,\n                                    return_tensors=self.Config.return_tensors)\n            \n\n\n        model_inputs = {\n            \"input_ids\": model_inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(),\n            \"decoder_input_ids\": labels[\"input_ids\"].squeeze(),\n            \"decoder_attention_mask\": labels[\"attention_mask\"].squeeze(),\n            \"labels\": labels[\"input_ids\"].squeeze(),\n        }\n        \n        model_inputs = {k: model_inputs[k] for k in self.required_columns}\n        \n        return model_inputs\n    \n  \n    def __getitem__(self, index):\n\n        return self.preprocess_function(self.dataset[index])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:53.643070Z","iopub.execute_input":"2021-11-03T07:27:53.643440Z","iopub.status.idle":"2021-11-03T07:27:53.656692Z","shell.execute_reply.started":"2021-11-03T07:27:53.643404Z","shell.execute_reply":"2021-11-03T07:27:53.654907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_datasets(Config, main_dataset, tokenizer):\n    \n    train_ds = MyXSum(Config, tokenizer, main_dataset, \"train\")\n    valid_ds = MyXSum(Config, tokenizer, main_dataset, \"validation\")\n    test_ds = MyXSum(Config, tokenizer, main_dataset, \"test\")\n    \n    data_dict = {\n        \"train\": train_ds,\n        \"validation\": valid_ds,\n        \"test\": test_ds,\n    }\n    return data_dict ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:53.658176Z","iopub.execute_input":"2021-11-03T07:27:53.658540Z","iopub.status.idle":"2021-11-03T07:27:53.676970Z","shell.execute_reply.started":"2021-11-03T07:27:53.658504Z","shell.execute_reply":"2021-11-03T07:27:53.676108Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"xsum_ds = load_dataset(\"csv\", data_files={\"train\": Config.train_files,\n                                          \"validation\": Config.valid_files,\n                                          \"test\": Config.test_files})\n\nos.makedirs(Config.datasets_main_dir, exist_ok=True)\n\ndata_dict = get_datasets(Config, xsum_ds, tokenizer)\n\ntorch.save(data_dict[\"train\"], Config.train_ds_path)\ntorch.save(data_dict[\"validation\"], Config.valid_ds_path)\ntorch.save(data_dict[\"test\"], Config.test_ds_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:27:53.678340Z","iopub.execute_input":"2021-11-03T07:27:53.678948Z","iopub.status.idle":"2021-11-03T07:28:01.651357Z","shell.execute_reply.started":"2021-11-03T07:27:53.678911Z","shell.execute_reply":"2021-11-03T07:28:01.650579Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7dc108abe4d1fe94/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"906e1d8e510b402ca81d84aef9a9ba15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abcfbf3f48674cb5ae48717c4775e888"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7dc108abe4d1fe94/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40db09900d134c82b7812750e0c265b9"}},"metadata":{}}]},{"cell_type":"code","source":"del xsum_ds\ndel data_dict\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:28:01.652764Z","iopub.execute_input":"2021-11-03T07:28:01.653117Z","iopub.status.idle":"2021-11-03T07:28:01.874521Z","shell.execute_reply.started":"2021-11-03T07:28:01.653079Z","shell.execute_reply":"2021-11-03T07:28:01.873636Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"89"},"metadata":{}}]},{"cell_type":"markdown","source":"# Initializing the Model: ","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass T2TDataCollator(DataCollatorWithPadding):\n    \n    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Take a list of samples from a Dataset and collate them into a batch.\n        Returns:\n            A dictionary of tensors\n        \"\"\"\n        input_ids = torch.stack([example['input_ids'] for example in batch])\n        labels = torch.stack([example['decoder_input_ids'] for example in batch])\n        labels[labels[:, :] == 0] = -100\n        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n        decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n        \n\n        return {\n            'input_ids': input_ids.squeeze(), \n            'attention_mask': attention_mask.squeeze(),\n            'labels': labels.squeeze(), \n            'decoder_attention_mask': decoder_attention_mask.squeeze()\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:28:01.875935Z","iopub.execute_input":"2021-11-03T07:28:01.876531Z","iopub.status.idle":"2021-11-03T07:28:01.886949Z","shell.execute_reply.started":"2021-11-03T07:28:01.876491Z","shell.execute_reply":"2021-11-03T07:28:01.886224Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_ds = torch.load(Config.train_ds_path)\nvalid_ds = torch.load(Config.valid_ds_path)\ntest_ds = torch.load(Config.test_ds_path)\n\nConfig.train_len = len(train_ds)\nConfig.valid_len = len(valid_ds)\nConfig.test_len = len(test_ds)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:28:01.888377Z","iopub.execute_input":"2021-11-03T07:28:01.888769Z","iopub.status.idle":"2021-11-03T07:28:02.176106Z","shell.execute_reply.started":"2021-11-03T07:28:01.888733Z","shell.execute_reply":"2021-11-03T07:28:02.175264Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_config  = AutoConfig.from_pretrained(Config.model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint, config=model_config)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:28:02.178018Z","iopub.execute_input":"2021-11-03T07:28:02.178664Z","iopub.status.idle":"2021-11-03T07:30:24.334876Z","shell.execute_reply.started":"2021-11-03T07:28:02.178623Z","shell.execute_reply":"2021-11-03T07:30:24.334008Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7030faa243d34ab5bfee84f318bb7d98"}},"metadata":{}}]},{"cell_type":"code","source":"#with deepspeed.zero.Init():\nmodel_config  = AutoConfig.from_pretrained(Config.model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint, config=model_config)\n    \nmodel.train()\n\nif is_torch_tpu_available():\n    WRAPPED_MODEL = xmp.MpModelWrapper(model)\nelif Config.deepspeed_config:\n    WRAPPED_MODEL = model\nelse:\n    WRAPPED_MODEL = model.to(\"cuda\")\n\ndata_collator = T2TDataCollator(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:24.336215Z","iopub.execute_input":"2021-11-03T07:30:24.336721Z","iopub.status.idle":"2021-11-03T07:30:37.698547Z","shell.execute_reply.started":"2021-11-03T07:30:24.336682Z","shell.execute_reply":"2021-11-03T07:30:37.696912Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"Config.epochs=4\n\nif not Config.total_train_steps:\n    Config.total_train_steps = (\n                (Config.train_len // (Config.train_batch_size * max(1, Config.tpu_cores if Config.tpu_cores else 0)))\n                // Config.gradient_accumulation_steps\n                * float(Config.epochs)\n            )\n\nprint(\"TOTAL TRAIN STEPS:\", Config.total_train_steps)\n\nConfig.eval_steps = int(Config.total_train_steps * Config.eval_ratio)\nprint(f\"Eval Ratio: {Config.eval_ratio}, Eval Steps: {Config.eval_steps}\")\n\nConfig.logging_steps = int(Config.total_train_steps * Config.logging_ratio)\nprint(f\"Logging Ratio: {Config.logging_ratio}, Logging Steps: {Config.logging_steps}\")\n\nConfig.save_steps = int(Config.total_train_steps * Config.saving_ratio)\nprint(f\"Save Ratio: {Config.saving_ratio}, Save Steps: {Config.save_steps}\")\n\nConfig.warmup_steps = int(Config.total_train_steps * Config.warmup_ratio)\nprint(f\"Warmup Ratio: {Config.warmup_ratio}, Warmup Steps: {Config.warmup_steps}\")\n\nConfig.optimizer_warmup_init=False","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.701960Z","iopub.execute_input":"2021-11-03T07:30:37.702326Z","iopub.status.idle":"2021-11-03T07:30:37.710834Z","shell.execute_reply.started":"2021-11-03T07:30:37.702290Z","shell.execute_reply":"2021-11-03T07:30:37.710049Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"TOTAL TRAIN STEPS: 100\nEval Ratio: 0.1, Eval Steps: 10\nLogging Ratio: 0.01, Logging Steps: 1\nSave Ratio: 0.2, Save Steps: 20\nWarmup Ratio: 0.1, Warmup Steps: 10\n","output_type":"stream"}]},{"cell_type":"code","source":"proxy_valid_ds = valid_ds\nproxy_test_ds = test_ds\n\nvalid_subset_len = Config.valid_batch_size * 3\ntest_subset_len = Config.test_batch_size * 2\n\n#if is_torch_tpu_available():\nproxy_valid_ds = torch.utils.data.Subset(valid_ds, range(valid_subset_len))\nproxy_test_ds = torch.utils.data.Subset(test_ds, range(test_subset_len))\n\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    proxy_valid_ds,\n    batch_size=Config.valid_batch_size,\n    drop_last=Config.drop_last_valid,\n    num_workers=Config.num_workers_valid,\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    proxy_test_ds,\n    batch_size=Config.test_batch_size,\n    drop_last=Config.drop_last_test,\n    num_workers=Config.num_workers_test,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.712010Z","iopub.execute_input":"2021-11-03T07:30:37.712498Z","iopub.status.idle":"2021-11-03T07:30:37.738567Z","shell.execute_reply.started":"2021-11-03T07:30:37.712462Z","shell.execute_reply":"2021-11-03T07:30:37.737675Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(len(valid_data_loader))\nprint(len(test_data_loader))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.739976Z","iopub.execute_input":"2021-11-03T07:30:37.740356Z","iopub.status.idle":"2021-11-03T07:30:37.750368Z","shell.execute_reply.started":"2021-11-03T07:30:37.740319Z","shell.execute_reply":"2021-11-03T07:30:37.749134Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"3\n2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    from prettytable import PrettyTable\n\n    def count_parameters(model):\n        table = PrettyTable([\"Modules\", \"Parameters\"])\n        total_params = 0\n        for name, parameter in model.named_parameters():\n            if not parameter.requires_grad: continue\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n        print(table)\n        print(f\"Total Trainable Params: {total_params}\")\n        return total_params\n\n    count_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T07:56:56.363717Z","iopub.execute_input":"2021-11-01T07:56:56.363982Z","iopub.status.idle":"2021-11-01T07:56:56.766823Z","shell.execute_reply.started":"2021-11-01T07:56:56.363956Z","shell.execute_reply":"2021-11-01T07:56:56.765889Z"}}},{"cell_type":"markdown","source":"### Rouge Before Training:","metadata":{}},{"cell_type":"code","source":"if Config.rouge_before_training:\n    gen_kwargs = {\n        \"num_beams\": Config.num_beams,\n        \"max_length\":Config.max_output_len,\n    }\n\n    valid_results = compute_rouge(model, tokenizer, valid_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n    test_results = compute_rouge(model, tokenizer, test_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n\n    print(\"VALID RESULTS: \", valid_results)\n    print(\"TEST RESULTS: \", test_results )\n\n    for idx, batch in enumerate(test_ds):\n\n        if is_torch_tpu_available():\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].unsqueeze(0),\n                \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0),\n            }\n        else:\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n                \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0).to(\"cuda\"),\n            }\n\n        labels = batch[\"labels\"]\n\n        temp_batch.update(gen_kwargs)\n\n        output_ids = model.generate(**temp_batch)\n        decoded_input=tokenizer.decode(temp_batch[\"input_ids\"].squeeze(), skip_special_tokens=False)\n        decoded_output = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=False)\n        decoded_labels = tokenizer.decode(labels.squeeze(), skip_special_tokens=False)\n\n        print(f\"{'-'*60}Example-{idx}{'-'*60}\")\n        print(\"INPUT: \", decoded_input[:400], \".\"*10, decoded_input[-100:], \"\\n\")\n        print(\"LABEL: \", decoded_labels, \"\\n\")\n        print(\"PREDICTION: \", decoded_output, \"\\n\")\n        print(\"\\n\")\n\n        if idx == 3:\n            break","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.751822Z","iopub.execute_input":"2021-11-03T07:30:37.752221Z","iopub.status.idle":"2021-11-03T07:30:37.763746Z","shell.execute_reply.started":"2021-11-03T07:30:37.752184Z","shell.execute_reply":"2021-11-03T07:30:37.762807Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### My Custom TPU Trainer which implements serialized saving when limited host memory available.","metadata":{}},{"cell_type":"code","source":"from transformers.file_utils import (\n    is_sagemaker_mp_enabled,\n)\nfrom transformers.trainer_utils import (\n    ShardedDDPOption,\n)\nfrom transformers.deepspeed import (\n    is_deepspeed_zero3_enabled\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    unwrap_model\n)\n\nclass TPUTrainer(Trainer):\n    \n    def __init__(self, limited_host_mem=None, **kwargs):\n        super().__init__(**kwargs)\n        self.limited_host_mem=limited_host_mem\n        \n    def _tune_save_checkpoint(self):\n        from ray import tune\n\n        if not self.use_tune_checkpoints:\n            return\n        with tune.checkpoint_dir(step=self.state.global_step) as checkpoint_dir:\n            output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n            self.save_model(output_dir)\n            if self.args.should_save:\n                self.state.save_to_json(os.path.join(output_dir, \"trainer_state.json\"))\n                torch.save(self.optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n    \n    def save_model(self, output_dir: Optional[str] = None, limited_host_mem=False):\n        \"\"\"\n        Will save the model, so you can reload it using :obj:`from_pretrained()`.\n\n        Will only save from the main process.\n        \"\"\"\n        if self.limited_host_mem:\n            limited_host_mem=self.limited_host_mem\n\n        if output_dir is None:\n            output_dir = self.args.output_dir\n\n        if is_torch_tpu_available():\n            self._save_tpu(output_dir, limited_host_mem)\n        elif is_sagemaker_mp_enabled():\n            # Calling the state_dict needs to be done on the wrapped model and on all processes.\n            state_dict = self.model_wrapped.state_dict()\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif (\n            ShardedDDPOption.ZERO_DP_2 in self.args.sharded_ddp or ShardedDDPOption.ZERO_DP_3 in self.args.sharded_ddp\n        ):\n            state_dict = self.model.state_dict()\n\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif self.deepspeed:\n\n            # this takes care of everything as long as we aren't under zero3\n            if self.args.should_save:\n                self._save(output_dir)\n\n            if is_deepspeed_zero3_enabled():\n                # It's too complicated to try to override different places where the weights dump gets\n                # saved, so since under zero3 the file is bogus, simply delete it. The user should\n                # either user deepspeed checkpoint to resume or to recover full weights use\n                # zero_to_fp32.py stored in the checkpoint.\n                if self.args.should_save:\n                    file = os.path.join(output_dir, WEIGHTS_NAME)\n                    if os.path.isfile(file):\n                        # logger.info(f\"deepspeed zero3: removing {file}, see zero_to_fp32.py to recover weights\")\n                        os.remove(file)\n\n                # now save the real model if stage3_gather_fp16_weights_on_model_save=True\n                # if false it will not be saved.\n                # This must be called on all ranks\n                self.deepspeed.save_fp16_model(output_dir, WEIGHTS_NAME)\n\n        elif self.args.should_save:\n            self._save(output_dir)\n    \n    def _save_tpu(self, output_dir: Optional[str] = None, limited_host_mem=False):\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n\n        if xm.is_master_ordinal():\n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        xm.rendezvous(\"saving_checkpoint\")\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                unwrap_model(self.model).save_pretrained(\n                    output_dir,\n                    save_config=self.args.should_save,\n                    state_dict=self.model.state_dict(),\n                    save_function=xm.xser if limited_host_mem else xm.save,\n                )\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                state_dict = self.model.state_dict()\n                if limited_host_mem:\n                    xm.xser(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n                else:\n                    xm.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            self.model.save_pretrained(output_dir, save_config=self.args.should_save,\n                                       save_function=xm.xser if limited_host_mem else xm.save)\n        if self.tokenizer is not None and self.args.should_save:\n            self.tokenizer.save_pretrained(output_dir)\n\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n        # If we are executing this function, we are the process zero, so we don't check for that.\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            self.model.save_pretrained(output_dir, state_dict=state_dict)\n        if self.tokenizer is not None:\n            self.tokenizer.save_pretrained(output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.765110Z","iopub.execute_input":"2021-11-03T07:30:37.765493Z","iopub.status.idle":"2021-11-03T07:30:37.789358Z","shell.execute_reply.started":"2021-11-03T07:30:37.765457Z","shell.execute_reply":"2021-11-03T07:30:37.788336Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    rouge_metric = load_metric(\"rouge\")\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    \n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n    \n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    del predictions\n    del decoded_preds\n    del labels\n    del decoded_labels\n    del eval_pred\n    del prediction_lens\n    del rouge_metric\n    gc.collect()\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.790560Z","iopub.execute_input":"2021-11-03T07:30:37.791128Z","iopub.status.idle":"2021-11-03T07:30:37.802618Z","shell.execute_reply.started":"2021-11-03T07:30:37.791091Z","shell.execute_reply":"2021-11-03T07:30:37.801756Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def get_trainer(Config=Config, optimizer=None, lr_scheduler=None):\n    training_args = TrainingArguments(\n        max_steps=Config.total_train_steps,\n        output_dir=Config.output_dir,\n        num_train_epochs=Config.epochs,\n        save_strategy=Config.save_strategy,\n        evaluation_strategy=Config.evaluation_strategy,\n        logging_dir=Config.logging_dir,\n        eval_steps=Config.eval_steps,\n        save_steps=Config.save_steps,\n        logging_steps=Config.logging_steps,\n        per_device_train_batch_size=Config.train_batch_size,\n        per_device_eval_batch_size=Config.valid_batch_size,\n        fp16=Config.fp16,\n        deepspeed=Config.deepspeed_config,\n        adafactor=Config.use_ada_factor\n    )\n    \n    if is_torch_tpu_available():\n        trainer = TPUTrainer(\n            limited_host_mem=Config.limited_host_mem,\n            model=model,\n            args=training_args,\n            compute_metrics=compute_metrics if Config.compute_metrics else None,\n            data_collator=data_collator,\n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            optimizers=(optimizer, lr_scheduler),    \n        )\n    else:\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            compute_metrics=compute_metrics if Config.compute_metrics else None,\n            data_collator=data_collator,\n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            optimizers=(optimizer, lr_scheduler),\n        )\n    return trainer\n\n\ndef get_optimizer_n_scheduler(model, Config=Config):\n    \n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    \n    grouped_params = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": Config.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        }\n    ]\n\n    optimizer = Adafactor(grouped_params, scale_parameter=Config.optimizer_scale_parameter, \n                          relative_step=Config.optimizer_relative_step, warmup_init=Config.optimizer_warmup_init,\n                          lr=Config.lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                   num_training_steps=Config.total_train_steps,\n                                                   num_warmup_steps =Config.warmup_steps)\n\n    return optimizer, lr_scheduler","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.806227Z","iopub.execute_input":"2021-11-03T07:30:37.806495Z","iopub.status.idle":"2021-11-03T07:30:37.819595Z","shell.execute_reply.started":"2021-11-03T07:30:37.806471Z","shell.execute_reply":"2021-11-03T07:30:37.818773Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def _mp_fn_boiler_plate(index, Config=Config):\n    \n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n\n    optimizer, lr_scheduler = get_optimizer_n_scheduler(model, Config)\n    \n    print(\"Loading datasets... \", end=\"\")\n\n    trainer = get_trainer(Config, optimizer, lr_scheduler)\n    trainer.place_model_on_device = False\n    trainer.train()\n\n    xser.save(trainer.model.state_dict(), Config.xser_save_path)\n    trainer.model.config.to_json_file(Config.config_json_path)\n    tokenizer.save_pretrained(Config.tokenizer_save_path)   ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.821133Z","iopub.execute_input":"2021-11-03T07:30:37.821552Z","iopub.status.idle":"2021-11-03T07:30:37.830404Z","shell.execute_reply.started":"2021-11-03T07:30:37.821515Z","shell.execute_reply":"2021-11-03T07:30:37.829435Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"run([\"rm\", \"-rf\", Config.xser_save_path])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:37.831671Z","iopub.execute_input":"2021-11-03T07:30:37.832095Z","iopub.status.idle":"2021-11-03T07:30:38.068362Z","shell.execute_reply.started":"2021-11-03T07:30:37.832057Z","shell.execute_reply":"2021-11-03T07:30:38.067356Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['rm', '-rf', 't5_large_xsum'], returncode=0)"},"metadata":{}}]},{"cell_type":"code","source":"Config.epochs=4\n\nif not Config.total_train_steps:\n    Config.total_train_steps = (\n                (Config.train_len // (Config.train_batch_size * max(1, Config.tpu_cores if Config.tpu_cores else 0)))\n                // Config.gradient_accumulation_steps\n                * float(Config.epochs)\n            )\n\nprint(\"TOTAL TRAIN STEPS:\", Config.total_train_steps)\n\nConfig.eval_steps = int(Config.total_train_steps * Config.eval_ratio)\nprint(f\"Eval Ratio: {Config.eval_ratio}, Eval Steps: {Config.eval_steps}\")\n\nConfig.logging_steps = int(Config.total_train_steps * Config.logging_ratio)\nprint(f\"Logging Ratio: {Config.logging_ratio}, Logging Steps: {Config.logging_steps}\")\n\nConfig.save_steps = int(Config.total_train_steps * Config.saving_ratio)\nprint(f\"Save Ratio: {Config.saving_ratio}, Save Steps: {Config.save_steps}\")\n\nConfig.warmup_steps = int(Config.total_train_steps * Config.warmup_ratio)\nprint(f\"Warmup Ratio: {Config.warmup_ratio}, Warmup Steps: {Config.warmup_steps}\")\n\nConfig.optimizer_warmup_init=False","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:38.070146Z","iopub.execute_input":"2021-11-03T07:30:38.070555Z","iopub.status.idle":"2021-11-03T07:30:38.080878Z","shell.execute_reply.started":"2021-11-03T07:30:38.070513Z","shell.execute_reply":"2021-11-03T07:30:38.079782Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"TOTAL TRAIN STEPS: 100\nEval Ratio: 0.1, Eval Steps: 10\nLogging Ratio: 0.01, Logging Steps: 1\nSave Ratio: 0.2, Save Steps: 20\nWarmup Ratio: 0.1, Warmup Steps: 10\n","output_type":"stream"}]},{"cell_type":"code","source":"Config.compute_metrics = False\n\nif is_torch_tpu_available():\n    _mp_fn = partial(_mp_fn_boiler_plate, Config=Config)\n    xmp.spawn(_mp_fn, start_method=\"fork\")\n\n    \"\"\"\n    ckpts_path = os.path.join(os.curdir, \"results\")\n    model_ckpts = os.listdir(ckpts_path)\n    ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    best_ckpt_num = max(ckpt_nums)\n    best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    tokenizer.save_pretrained(best_ckpt_path)\n    \"\"\"\n\nelse:\n    if Config.deepspeed_config and Config.use_ds_optimizer_scheduler:\n        optimizer = None\n        lr_scheduler = None\n    else:\n        optimizer, lr_scheduler = get_optimizer_n_scheduler(WRAPPED_MODEL, Config)\n        #if Config.deepspeed_config\n        \n    #print(optimizer)\n    trainer = get_trainer(Config, optimizer, lr_scheduler)\n    #print(trainer.optimizer)\n    trainer.train()\n    \n    #ckpts_path = os.path.join(os.curdir, \"results\")\n    #model_ckpts = os.listdir(ckpts_path)\n    #ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    #best_ckpt_num = max(ckpt_nums)\n    #best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    #tokenizer.save_pretrained(best_ckpt_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:30:38.082318Z","iopub.execute_input":"2021-11-03T07:30:38.082925Z","iopub.status.idle":"2021-11-03T07:35:51.737677Z","shell.execute_reply.started":"2021-11-03T07:30:38.082886Z","shell.execute_reply":"2021-11-03T07:35:51.736682Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[2021-11-03 07:30:38,152] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n","output_type":"stream"},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nUsing amp fp16 backend\n","output_type":"stream"},{"name":"stdout","text":"[2021-11-03 07:30:43,797] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.5.6+426dd2b, git-hash=426dd2b, git-branch=master\n[2021-11-03 07:30:45,218] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed groups\n[2021-11-03 07:30:45,219] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n[2021-11-03 07:30:45,221] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n[2021-11-03 07:30:45,222] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n[2021-11-03 07:30:45,223] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n[2021-11-03 07:30:45,295] [INFO] [engine.py:208:__init__] DeepSpeed Flops Profiler Enabled: False\n[2021-11-03 07:30:45,296] [INFO] [engine.py:871:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n[2021-11-03 07:30:45,297] [INFO] [engine.py:876:_configure_optimizer] Using client Optimizer as basic optimizer\n[2021-11-03 07:30:45,345] [INFO] [engine.py:893:_configure_optimizer] DeepSpeed Basic Optimizer = Adafactor\n[2021-11-03 07:30:45,345] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 unfused optimizer with dynamic loss scale\n[2021-11-03 07:30:45,346] [INFO] [unfused_optimizer.py:39:__init__] Fused Lamb Legacy : False \n[2021-11-03 07:30:46,573] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adafactor\n[2021-11-03 07:30:46,574] [INFO] [engine.py:605:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n[2021-11-03 07:30:46,576] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5868cc3bd0>\n[2021-11-03 07:30:46,577] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[0.0, 0.0]\n[2021-11-03 07:30:46,581] [INFO] [config.py:958:print] DeepSpeedEngine configuration:\n[2021-11-03 07:30:46,583] [INFO] [config.py:962:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2021-11-03 07:30:46,583] [INFO] [config.py:962:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n[2021-11-03 07:30:46,585] [INFO] [config.py:962:print]   allreduce_always_fp32 ........ False\n[2021-11-03 07:30:46,586] [INFO] [config.py:962:print]   amp_enabled .................. False\n[2021-11-03 07:30:46,587] [INFO] [config.py:962:print]   amp_params ................... False\n[2021-11-03 07:30:46,588] [INFO] [config.py:962:print]   bfloat16_enabled ............. False\n[2021-11-03 07:30:46,589] [INFO] [config.py:962:print]   checkpoint_tag_validation_enabled  True\n[2021-11-03 07:30:46,590] [INFO] [config.py:962:print]   checkpoint_tag_validation_fail  False\n[2021-11-03 07:30:46,591] [INFO] [config.py:962:print]   curriculum_enabled ........... False\n[2021-11-03 07:30:46,592] [INFO] [config.py:962:print]   curriculum_params ............ False\n[2021-11-03 07:30:46,592] [INFO] [config.py:962:print]   dataloader_drop_last ......... False\n[2021-11-03 07:30:46,593] [INFO] [config.py:962:print]   disable_allgather ............ False\n[2021-11-03 07:30:46,594] [INFO] [config.py:962:print]   dump_state ................... False\n[2021-11-03 07:30:46,595] [INFO] [config.py:962:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n[2021-11-03 07:30:46,596] [INFO] [config.py:962:print]   eigenvalue_enabled ........... False\n[2021-11-03 07:30:46,596] [INFO] [config.py:962:print]   eigenvalue_gas_boundary_resolution  1\n[2021-11-03 07:30:46,597] [INFO] [config.py:962:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2021-11-03 07:30:46,598] [INFO] [config.py:962:print]   eigenvalue_layer_num ......... 0\n[2021-11-03 07:30:46,599] [INFO] [config.py:962:print]   eigenvalue_max_iter .......... 100\n[2021-11-03 07:30:46,599] [INFO] [config.py:962:print]   eigenvalue_stability ......... 1e-06\n[2021-11-03 07:30:46,600] [INFO] [config.py:962:print]   eigenvalue_tol ............... 0.01\n[2021-11-03 07:30:46,601] [INFO] [config.py:962:print]   eigenvalue_verbose ........... False\n[2021-11-03 07:30:46,602] [INFO] [config.py:962:print]   elasticity_enabled ........... False\n[2021-11-03 07:30:46,602] [INFO] [config.py:962:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2021-11-03 07:30:46,603] [INFO] [config.py:962:print]   fp16_enabled ................. True\n[2021-11-03 07:30:46,604] [INFO] [config.py:962:print]   fp16_master_weights_and_gradients  False\n[2021-11-03 07:30:46,605] [INFO] [config.py:962:print]   fp16_mixed_quantize .......... False\n[2021-11-03 07:30:46,605] [INFO] [config.py:962:print]   global_rank .................. 0\n[2021-11-03 07:30:46,606] [INFO] [config.py:962:print]   gradient_accumulation_steps .. 1\n[2021-11-03 07:30:46,607] [INFO] [config.py:962:print]   gradient_clipping ............ 1.0\n[2021-11-03 07:30:46,608] [INFO] [config.py:962:print]   gradient_predivide_factor .... 1.0\n[2021-11-03 07:30:46,608] [INFO] [config.py:962:print]   initial_dynamic_scale ........ 65536\n[2021-11-03 07:30:46,609] [INFO] [config.py:962:print]   loss_scale ................... 0\n[2021-11-03 07:30:46,610] [INFO] [config.py:962:print]   memory_breakdown ............. False\n[2021-11-03 07:30:46,611] [INFO] [config.py:962:print]   optimizer_legacy_fusion ...... False\n[2021-11-03 07:30:46,611] [INFO] [config.py:962:print]   optimizer_name ............... None\n[2021-11-03 07:30:46,612] [INFO] [config.py:962:print]   optimizer_params ............. None\n[2021-11-03 07:30:46,613] [INFO] [config.py:962:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n[2021-11-03 07:30:46,614] [INFO] [config.py:962:print]   pld_enabled .................. False\n[2021-11-03 07:30:46,614] [INFO] [config.py:962:print]   pld_params ................... False\n[2021-11-03 07:30:46,615] [INFO] [config.py:962:print]   prescale_gradients ........... False\n[2021-11-03 07:30:46,616] [INFO] [config.py:962:print]   quantize_change_rate ......... 0.001\n[2021-11-03 07:30:46,617] [INFO] [config.py:962:print]   quantize_groups .............. 1\n[2021-11-03 07:30:46,617] [INFO] [config.py:962:print]   quantize_offset .............. 1000\n[2021-11-03 07:30:46,618] [INFO] [config.py:962:print]   quantize_period .............. 1000\n[2021-11-03 07:30:46,619] [INFO] [config.py:962:print]   quantize_rounding ............ 0\n[2021-11-03 07:30:46,619] [INFO] [config.py:962:print]   quantize_start_bits .......... 16\n[2021-11-03 07:30:46,620] [INFO] [config.py:962:print]   quantize_target_bits ......... 8\n[2021-11-03 07:30:46,621] [INFO] [config.py:962:print]   quantize_training_enabled .... False\n[2021-11-03 07:30:46,622] [INFO] [config.py:962:print]   quantize_type ................ 0\n[2021-11-03 07:30:46,622] [INFO] [config.py:962:print]   quantize_verbose ............. False\n[2021-11-03 07:30:46,623] [INFO] [config.py:962:print]   scheduler_name ............... None\n[2021-11-03 07:30:46,624] [INFO] [config.py:962:print]   scheduler_params ............. None\n[2021-11-03 07:30:46,625] [INFO] [config.py:962:print]   sparse_attention ............. None\n[2021-11-03 07:30:46,625] [INFO] [config.py:962:print]   sparse_gradients_enabled ..... False\n[2021-11-03 07:30:46,626] [INFO] [config.py:962:print]   steps_per_print .............. 2000\n[2021-11-03 07:30:46,627] [INFO] [config.py:962:print]   tensorboard_enabled .......... False\n[2021-11-03 07:30:46,628] [INFO] [config.py:962:print]   tensorboard_job_name ......... DeepSpeedJobName\n[2021-11-03 07:30:46,628] [INFO] [config.py:962:print]   tensorboard_output_path ...... \n[2021-11-03 07:30:46,629] [INFO] [config.py:962:print]   train_batch_size ............. 10\n[2021-11-03 07:30:46,630] [INFO] [config.py:962:print]   train_micro_batch_size_per_gpu  10\n[2021-11-03 07:30:46,631] [INFO] [config.py:962:print]   use_quantizer_kernel ......... False\n[2021-11-03 07:30:46,631] [INFO] [config.py:962:print]   wall_clock_breakdown ......... False\n[2021-11-03 07:30:46,632] [INFO] [config.py:962:print]   world_size ................... 1\n[2021-11-03 07:30:46,633] [INFO] [config.py:962:print]   zero_allow_untested_optimizer  True\n[2021-11-03 07:30:46,634] [INFO] [config.py:962:print]   zero_config .................. {\n    \"stage\": 0, \n    \"contiguous_gradients\": true, \n    \"reduce_scatter\": true, \n    \"reduce_bucket_size\": 5.000000e+08, \n    \"allgather_partitions\": true, \n    \"allgather_bucket_size\": 5.000000e+08, \n    \"overlap_comm\": false, \n    \"load_from_fp32_weights\": true, \n    \"elastic_checkpoint\": true, \n    \"offload_param\": null, \n    \"offload_optimizer\": null, \n    \"sub_group_size\": 1.000000e+09, \n    \"prefetch_bucket_size\": 5.000000e+07, \n    \"param_persistence_threshold\": 1.000000e+05, \n    \"max_live_parameters\": 1.000000e+09, \n    \"max_reuse_distance\": 1.000000e+09, \n    \"gather_fp16_weights_on_model_save\": false, \n    \"ignore_unused_parameters\": true, \n    \"round_robin_gradients\": false, \n    \"legacy_stage1\": false\n}\n[2021-11-03 07:30:46,635] [INFO] [config.py:962:print]   zero_enabled ................. False\n[2021-11-03 07:30:46,635] [INFO] [config.py:962:print]   zero_optimization_stage ...... 0\n[2021-11-03 07:30:46,636] [INFO] [config.py:969:print]   json = {\n    \"fp16\": {\n        \"enabled\": true, \n        \"loss_scale\": 0, \n        \"loss_scale_window\": 1000, \n        \"initial_scale_power\": 16, \n        \"hysteresis\": 2, \n        \"min_loss_scale\": 1\n    }, \n    \"zero_allow_untested_optimizer\": true, \n    \"\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    zero_optimization\": {\n        \"stage\": 3, \n        \"offload_optimizer\": {\n            \"device\": \"cpu\", \n            \"pin_memory\": false\n        }, \n        \"offload_param\": {\n            \"device\": \"cpu\", \n            \"pin_memory\": true\n        }, \n        \"overlap_comm\": false, \n        \"contiguous_gradients\": true, \n        \"sub_group_size\": 1.000000e+05, \n        \"allgather_bucket_size\": 1.000000e+08, \n        \"reduce_bucket_size\": \"auto\", \n        \"stage3_prefetch_bucket_size\": \"auto\", \n        \"stage3_param_persistence_threshold\": \"auto\", \n        \"stage3_max_live_parameters\": 1.000000e+05, \n        \"stage3_max_reuse_distance\": 1.000000e+05, \n        \"stage3_gather_fp16_weights_on_model_save\": true\n    }, \n    \"gradient_accumulation_steps\": 1, \n    \"gradient_clipping\": 1.0, \n    \"steps_per_print\": 2.000000e+03, \n    \"train_batch_size\": 10, \n    \"train_micro_batch_size_per_gpu\": 10, \n    \"wall_clock_breakdown\": false\n}\nUsing /root/.cache/torch_extensions as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/utils...\nEmitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\nBuilding extension module utils...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 204017\n  Num Epochs = 1\n  Instantaneous batch size per device = 10\n  Total train batch size (w. parallel, distributed & accumulation) = 10\n  Gradient Accumulation steps = 1\n  Total optimization steps = 100\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"Loading extension module utils...\nTime to load utils op: 14.274823904037476 seconds\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprikmm\u001b[0m (use `wandb login --relogin` to force relogin)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    Syncing run <strong><a href=\"https://wandb.ai/prikmm/huggingface/runs/22tojnc5\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/prikmm/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "},"metadata":{}},{"name":"stdout","text":"[2021-11-03 07:31:13,617] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 0\n[2021-11-03 07:31:13,618] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0\n[2021-11-03 07:31:13,620] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 04:37, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>8.828100</td>\n      <td>4.085938</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.210000</td>\n      <td>1.090820</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.264600</td>\n      <td>1.089844</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.118200</td>\n      <td>1.064453</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.153300</td>\n      <td>1.055664</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.091800</td>\n      <td>1.032227</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.971700</td>\n      <td>1.007812</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.129900</td>\n      <td>0.998535</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.004900</td>\n      <td>0.991211</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.857900</td>\n      <td>0.982422</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[2021-11-03 07:31:15,035] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 1\n[2021-11-03 07:31:15,039] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0\n[2021-11-03 07:31:15,040] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n[2021-11-03 07:31:16,429] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 2\n[2021-11-03 07:31:16,430] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n[2021-11-03 07:31:16,432] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n[2021-11-03 07:31:17,822] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 3\n[2021-11-03 07:31:17,823] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n[2021-11-03 07:31:17,826] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n[2021-11-03 07:31:19,172] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 4\n[2021-11-03 07:31:19,173] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n[2021-11-03 07:31:19,177] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n[2021-11-03 07:31:20,560] [INFO] [unfused_optimizer.py:275:_update_scale] Grad overflow on iteration: 5\n[2021-11-03 07:31:20,560] [INFO] [unfused_optimizer.py:277:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n[2021-11-03 07:31:20,561] [INFO] [unfused_optimizer.py:202:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n","output_type":"stream"},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 10\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"if is_torch_tpu_available():\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_torch_tpu_available():\n    tokenizer = AutoTokenizer.from_pretrained(Config.tokenizer_save_path,\n                                              local_files_only=True,\n                                              use_fast=Config.use_fast_tokenizer)\n    \n    model = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint)\n    state_dict = xser.load(Config.xser_save_path)\n    model.load_state_dict(state_dict)\nelse:\n    ckpts_path = os.path.join(os.curdir, \"results\")\n    model_ckpts = os.listdir(ckpts_path)\n    ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    best_ckpt_num = max(ckpt_nums)\n    best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    tokenizer.save_pretrained(best_ckpt_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(best_ckpt_path, local_files_only=True)\n    model = model.to(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rouge After Training:","metadata":{}},{"cell_type":"code","source":"gen_kwargs = {\n    \"num_beams\": Config.num_beams,\n    \"max_length\":Config.max_output_len,\n}\n\nvalid_results = compute_rouge(model, tokenizer, valid_data_loader, gen_kwargs=gen_kwargs, Config=Config)\ntest_results = compute_rouge(model, tokenizer, test_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n\nprint(\"VALID RESULTS: \", valid_results)\nprint(\"TEST RESULTS: \", test_results)\n\nfor idx, batch in enumerate(test_ds):\n    \n    if is_torch_tpu_available():\n        temp_batch={\n            \"input_ids\": batch[\"input_ids\"].unsqueeze(0),\n            \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0),\n        }\n    else:\n        temp_batch={\n            \"input_ids\": batch[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n            \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0).to(\"cuda\"),\n        }\n        \n    labels = batch[\"labels\"]\n    \n    temp_batch.update(gen_kwargs)\n\n    output_ids = model.generate(**temp_batch)\n    decoded_input=tokenizer.decode(temp_batch[\"input_ids\"].squeeze(), skip_special_tokens=False)\n    decoded_output = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=False)\n    decoded_labels = tokenizer.decode(labels.squeeze(), skip_special_tokens=False)\n\n    print(f\"{'-'*60}Example-{idx}{'-'*60}\")\n    print(\"INPUT: \", decoded_input[:400], \".\"*10, decoded_input[-100:], \"\\n\")\n    print(\"LABEL: \", decoded_labels, \"\\n\")\n    print(\"PREDICTION: \", decoded_output, \"\\n\")\n    print(\"\\n\")\n\n    if idx == 3:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}